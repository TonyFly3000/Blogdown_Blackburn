---
title: (3/7)TensorFlow by Udacity:Conversational neural network(CNN)
author: ''
date: '2000-01-01'
slug: 3-7-tensorflow-by-udacity-conversational-neural-network-cnn
categories: [R]
tags: [R]
description: '(3/7)TensorFlow by Udacity:Conversational neural network(CNN)'
topics: []
---



<pre class="python"><code>import tensorflow_datasets as tfds
tfds.disable_progress_bar()

# Helper libraries
import math
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf</code></pre>
<pre class="python"><code>import tensorflow as tf
print(tf.__version__)</code></pre>
<pre class="python"><code># data
dataset, metadata = tfds.load(&#39;fashion_mnist&#39;, as_supervised=True, with_info=True)
train_dataset, test_dataset = dataset[&#39;train&#39;], dataset[&#39;test&#39;]</code></pre>
<pre class="python"><code>class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;,
               &#39;Sandal&#39;,      &#39;Shirt&#39;,   &#39;Sneaker&#39;,  &#39;Bag&#39;,   &#39;Ankle boot&#39;]</code></pre>
<pre class="python"><code>num_train_examples = metadata.splits[&#39;train&#39;].num_examples
num_test_examples = metadata.splits[&#39;test&#39;].num_examples
print(&quot;Number of training examples: {}&quot;.format(num_train_examples))
print(&quot;Number of test examples:     {}&quot;.format(num_test_examples))</code></pre>
<pre class="python"><code>def normalize(images, labels):
  images = tf.cast(images, tf.float32)
  images /= 255
  return images, labels

# The map function applies the normalize function to each element in the train
# and test datasets
train_dataset =  train_dataset.map(normalize)
test_dataset  =  test_dataset.map(normalize)

# The first time you use the dataset, the images will be loaded from disk
# Caching will keep them in memory, making training faster
train_dataset =  train_dataset.cache()
test_dataset  =  test_dataset.cache()</code></pre>
<pre class="python"><code># model

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), padding=&#39;same&#39;, activation=tf.nn.relu,
                           input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D((2, 2), strides=2),
    tf.keras.layers.Conv2D(64, (3,3), padding=&#39;same&#39;, activation=tf.nn.relu),
    tf.keras.layers.MaxPooling2D((2, 2), strides=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)
])

model.compile(optimizer=&#39;adam&#39;,
              loss=&#39;sparse_categorical_crossentropy&#39;,
              metrics=[&#39;accuracy&#39;])</code></pre>
<pre class="python"><code># traninning
BATCH_SIZE = 32
train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)

model.fit(train_dataset, epochs=3, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),verbose=False)</code></pre>
<pre class="python"><code># result
test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))
print(&#39;Accuracy on test dataset:&#39;, test_accuracy)</code></pre>
<pre class="python"><code>for test_images, test_labels in test_dataset.take(1):
  test_images = test_images.numpy()
  test_labels = test_labels.numpy()
  predictions = model.predict(test_images)</code></pre>
<pre class="python"><code>predictions.shape
predictions[0]
np.argmax(predictions[0])
test_labels[0]</code></pre>
