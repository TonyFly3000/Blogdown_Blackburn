Spicy_flag=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken')
)
data=ramen_ratings_02%>%
ggplot(data, aes(x = Spicy_flag , fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02%>%mutate(country=fct_lump(country,8))
ggplot(data, aes(x = country, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar()+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Spicy_flag=if_else(Spicy>0,1,0)
Beef_flag=str_detect(variety,'Beef'),
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Spicy_flag=if_else(Spicy>0,1,0),
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken')
)
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar()+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
test$predict_stars_target_prob<-predict(tree, test)
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[1]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,1]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test
test$predict_stars_target<-predict(tree, test, type = 'class')
table_mat <- table(test$stars_target, test$predict_stars_target)
table_mat
test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
top_50_test=head(test,50)
top_50_test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
library("ROCR")
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
library(ROCR)
pred <- prediction(predict(tree, type = "prob")[, 2], test$stars_target)
Pred.cart = predict(tree, newdata = test, type = "prob")[,2]
Pred2 = prediction(Pred.cart, test$stars_target)
plot(performance(Pred2, "tpr", "fpr"))
abline(0, 1, lty = 2)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
getwd()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/public/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("./public/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv(".post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("./post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post\data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
blogdown:::serve_site()
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
plastic_waste <- clean_dataset(coast_vs_waste) %>%  # 清洗数据 coast_vs_waste
select(-total_population_gapminder) %>%           # 删除total_population_gapminder变量
inner_join(clean_dataset(mismanaged_vs_gdp) %>%   # 清洗数据 mismanaged_vs_gdp
select(-total_population_gapminder), # 删除total_population_gapminder变量
by = c("country", "country_code")) %>% # inner join by "country", "country_code"
inner_join(clean_dataset(waste_vs_gdp),           # 清洗数据 waste_vs_gdp
by = c("country", "country_code")) %>% # inner join by "country", "country_code"
# 选变量
select(country,
country_code,
mismanaged_waste = mismanaged_plastic_waste_tonnes,
coastal_population,
total_population = total_population_gapminder,
mismanaged_per_capita = per_capita_mismanaged_plastic_waste_kilograms_per_person_per_day,
plastic_waste_per_capita=per_capita_plastic_waste_kilograms_per_person_per_day,
gdp_per_capita = gdp_per_capita_ppp_constant_2011_international_rate) %>%
mutate(gdp_per_capita=round(gdp_per_capita))%>%
filter(!is.na(mismanaged_waste),country!='Trinidad and Tobago')%>%                 # 选mismanaged_waste 非空的记录
mutate(gdp_per_capita_group=cut_number(gdp_per_capita/1000, n = 4) # 按gdp_per_capita 排序 将国家分为 4组
,mismanaged_per_capita_rate=mismanaged_per_capita/plastic_waste_per_capita,
managed_per_capita_rate=1-mismanaged_per_capita_rate
)
global_plastics_production%>%clean_names() %>%filter(year>2000) %>%
ggplot(aes(x=year,y=global_plastics_production_million_tonnes_tonnes/1000000)) +geom_point()+geom_line()+
labs(x = "年",
y = "全球制造的塑料垃圾（百万吨）",
#color = "Coastal population",
title = "人类制造越来越多的垃圾",
subtitle = "每年制造近4亿万吨塑料垃圾",
caption ="统计时间:2000-2015年;数据源:ourworldindata.org
@Tony Duan"
)+theme(plot.title = element_text(hjust = 0.5))
global_plastics_production
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
library(keras)
library(reticulate)
tensorflow::tf_config()
keras:::keras_version()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
img <- x_train[1, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
x_train[1, , ][1]
x_train[1, , ][1:784]
length(x_train[1, , ])
x_train[1, , ]
200/255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
dim(x_train)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
dim(y_train)
head(y_train)
?compile
summary(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
256*784
256*128
255*784
257*784
256*785
128*785
257*128
200960+32896+1290
784*256
200960-200704
128*10
blogdown:::serve_site()
blogdown:::serve_site()
model_score=cbind(Naive_score,decision_tree_score,randon_forest_score,tensorflow_DNN_score)
library(keras)
library(reticulate)
tensorflow::tf_config()
keras:::keras_version()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
img <- x_train[1, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
class(x_train)
class(y_train)
class(x_test)
class(y_test)
str(x_train)
str(y_train)
dim(x_train)
dim(y_train)
dim(x_test)
dim(y_test)
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
dim(x_train)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
dim(y_train)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
head(y_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.2
)
plot(history)
y_test_final=mnist$test$y
table(y_test_final)
Naive_score=1000/length(y_test_final)
Naive_score
library(tidyverse)
library(keras)
library(rpart)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
x_train <- x_train / 255
x_test <- x_test / 255
all_data_train=as.data.frame(cbind(x_train, y_train))
d_tree <- rpart(y_train ~ .,method="class", data=all_data_train)
prediction<- predict(d_tree, newdata = as.data.frame(x_test), type = "class")
prediction=as.data.frame(prediction)
all_prediction=as.data.frame(cbind(prediction,y_test))%>%mutate(correct=if_else(prediction==y_test,1,0))
pred <- predict(rf, x_test)
pred <- predict(rf, x_test)
pred <- predict(rf, x_test)
pred <- predict(d_tree, x_test)
pred <- predict(d_tree, as.data.frame(x_test))
decision_tree_score=mean(pred == all_data_test$y_test)
pred <- predict(d_tree, as.data.frame(x_test))
decision_tree_score=mean(pred == y_test)
pred <- predict(d_tree, as.data.frame(x_test))
all_data_test=as.data.frame(cbind(x_test, y_test))
decision_tree_score=mean(pred == all_data_test$y_test)
decision_tree_score
decision_tree_score
prediction<- predict(d_tree, newdata = as.data.frame(x_test), type = "class")
prediction=as.data.frame(prediction)
all_prediction=as.data.frame(cbind(prediction,y_test))%>%mutate(correct=if_else(prediction==y_test,1,0))
decision_tree_score=sum(all_prediction$correct)/nrow(all_prediction)
decision_tree_score
library(tidyverse)
library(keras)
library(randomForest)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
x_train <- x_train / 255
x_test <- x_test / 255
all_data_train=as.data.frame(cbind(x_train, y_train))
all_data_train$y_train=factor(all_data_train$y_train)
all_data_test=as.data.frame(cbind(x_test, y_test))
numTrees = 5
rf <- randomForest(y_train ~ .,data=all_data_train,ntree=numTrees,importance = TRUE)
pred <- predict(rf, x_test)
randon_forest_score=mean(pred == all_data_test$y_test)
randon_forest_score
model_score=cbind(Naive_score,decision_tree_score,randon_forest_score,tensorflow_DNN_score)
tensorflow_DNN_score <- model %>% evaluate(x_test,y_test) %>% select(acc)
tensorflow_DNN_score <- model %>% evaluate(x_test,y_test)
tensorflow_DNN_score <- model %>% evaluate(x_test,y_test)
library(keras)
library(reticulate)
tensorflow::tf_config()
keras:::keras_version()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
img <- x_train[1, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
class(x_train)
class(y_train)
class(x_test)
class(y_test)
str(x_train)
str(y_train)
dim(x_train)
dim(y_train)
dim(x_test)
dim(y_test)
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
dim(x_train)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
dim(y_train)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
head(y_train)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.2
)
plot(history)
tensorflow_DNN_score
tensorflow_DNN_score <- model %>% evaluate(x_test,y_test)
tensorflow_DNN_score
tensorflow_DNN_score
tensorflow_DNN_score$acc
tensorflow_DNN_score=tensorflow_DNN_score$acc
tensorflow_DNN_score=tensorflow_DNN_score$acc
tensorflow_DNN_score
model_score=cbind(Naive_score,decision_tree_score,randon_forest_score,tensorflow_DNN_score)
model_score
glimpse(model_score)
as.data.frame(model_score)
aa=as.data.frame(model_score)
glimpse(aa)
model_score=rbind(Naive_score,decision_tree_score,randon_forest_score,tensorflow_DNN_score)
model_score
as.data.frame(model_score)%>%ggplot()
as.data.frame(model_score)
gg=model_score_df%>%ggplot(aes(x=model,y=V1)+geom_bar(stat = 'identity')
model_score_df=as.data.frame(model_score)
model_score_df=as.data.frame(model_score)
model_score_df$model <- row.names(model_score_df)
gg=ggplot(data=model_score_df,aes(x=model,y=V1)+geom_bar(stat = 'identity')
gg=ggplot(data=model_score_df,aes(x=model,y=V1)+geom_bar(stat = 'identity')
model_score_df=as.data.frame(model_score)
model_score_df$model <- row.names(model_score_df)
gg=ggplot(data=model_score_df,aes(x=model,y=V1))+geom_bar(stat = 'identity')
gg
model_score_df=as.data.frame(model_score)
model_score_df$model <- row.names(model_score_df)
gg=ggplot(data=model_score_df,aes(x=model,y=V1))+geom_bar(stat = 'identity') +coord_flip()
gg
