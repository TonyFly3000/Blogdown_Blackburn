input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 20))
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 100))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
model %>% summary()
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
epochs <- 500
# Fit the model and store training stats
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 5))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 5))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 8))
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(xlim = c(0, 150), ylim = c(0, 5))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 8))
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 8))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(xlim = c(0, 100), ylim = c(0, 8))
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
model %>% summary()
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
epochs <- 500
# Fit the model and store training stats
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 8))
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(xlim = c(0, 100), ylim = c(0, 8))
# mae in test with tensorflow DNN
round(mae * 1000)
# mae in test with tensorflow DNN
history
# Predict
test_predictions <- model %>% predict(test_data)
all_test_data$DNN_test_pred=test_predictions
all_test_data$diff=all_test_data$DNN_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Test data is *not* used when calculating the mean and std.
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
train_data[1, ] # First training sample, normalized
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
model %>% summary()
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat("\n")
cat(".")
}
)
epochs <- 500
# Fit the model and store training stats
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 8))
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(xlim = c(0, 100), ylim = c(0, 8))
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
train_data[1, ] # First training sample, normalized
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_rmsprop(),
metrics = list("mean_absolute_error")
)
history <- model %>% fit(
train_data,
train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(xlim = c(0, 100), ylim = c(0, 8))
history
# Predict
test_predictions <- model %>% predict(test_data)
all_test_data$DNN_test_pred=test_predictions
all_test_data$diff=all_test_data$DNN_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
all_train_data=cbind(train_data,train_labels)
all_test_data=cbind(test_data,test_labels)
all_train_data=as.data.frame(all_train_data)
all_test_data=as.data.frame(all_test_data)
all_test_data=all_test_data %>% rename(train_labels=test_labels)
dnn_estimator_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
model <- dnn_regressor(
feature_columns = cols,
hidden_units = c(10, 20, 10)
)
# train the model
model %>% train(dnn_estimator_input_fn(all_train_data, num_epochs = 20))
model %>% predict(dnn_estimator_input_fn(all_test_data))
dnn_estimator_test=model %>% predict(dnn_estimator_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(dnn_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
all_train_data=cbind(train_data,train_labels)
all_test_data=cbind(test_data,test_labels)
all_train_data=as.data.frame(all_train_data)
all_test_data=as.data.frame(all_test_data)
all_test_data=all_test_data %>% rename(train_labels=test_labels)
lm_model = lm(train_labels~., data = all_train_data) #Create the linear regression
summary(lm_model) #Review the results
all_test_data$test_Pred <- predict(lm_model, all_test_data)
all_test_data$diff=all_test_data$test_Pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
all_train_data=cbind(train_data,train_labels)
all_test_data=cbind(test_data,test_labels)
all_train_data=as.data.frame(all_train_data)
all_test_data=as.data.frame(all_test_data)
all_test_data=all_test_data %>% rename(train_labels=test_labels)
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 100))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 20))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Normalize training data
train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center")
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
all_train_data=cbind(train_data,train_labels)
all_test_data=cbind(test_data,test_labels)
all_train_data=as.data.frame(all_train_data)
all_test_data=as.data.frame(all_test_data)
all_test_data=all_test_data %>% rename(train_labels=test_labels)
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 20))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
boston_housing <- dataset_boston_housing()
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# Normalize training data
#train_data <- scale(train_data)
# Use means and standard deviations from training set to normalize test set
#col_means_train <- attr(train_data, "scaled:center")
#col_stddevs_train <- attr(train_data, "scaled:scale")
#test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
all_train_data=cbind(train_data,train_labels)
all_test_data=cbind(test_data,test_labels)
all_train_data=as.data.frame(all_train_data)
all_test_data=as.data.frame(all_test_data)
all_test_data=all_test_data %>% rename(train_labels=test_labels)
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 20))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
lr_input_fn  <- function(data, num_epochs = 1) {
input_fn(data,
features = c('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13'),
response = 'train_labels',
batch_size = 32,
num_epochs = num_epochs)
}
cols <- feature_columns(
column_numeric('V1' ,'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8' ,'V9', 'V10', 'V11', 'V12', 'V13')
)
lr_estimator_model <- linear_regressor(feature_columns = cols)
# train the model
lr_estimator_model %>% train(lr_input_fn(all_train_data, num_epochs = 100))
lr_estimator_test=lr_estimator_model %>% predict(lr_input_fn(all_test_data))
all_test_data$lr_estimator_test_pred=unlist(lr_estimator_test)
all_test_data$diff=all_test_data$lr_estimator_test_pred-all_test_data$train_labels
# mae in test with lm()
round(mean(abs(all_test_data$diff))*1000)
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
knitr::opts_chunk$set(echo = TRUE)
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
import tensorflow as tf
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
