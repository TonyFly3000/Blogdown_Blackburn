library(dbplot)
library(bigrquery)
library(dplyr, warn.conflicts = FALSE)
library(dbplyr, warn.conflicts = FALSE)
library(modeldb)
library(leaflet)
con <- dbConnect(
bigquery(),
project = "bigquery-public-data",
dataset = "austin_311",
billing = "rstudio-bigquery-event",
use_legacy_sql = FALSE
)
service <- tbl(con, "311_service_requests")
con <- dbConnect(
bigrquery::bigquery(),
project = "publicdata",
dataset = "samples",
billing = 'tony-229812'
)
con
dbListTables(con)
install.packages('curl')
install.packages("curl")
library(DBI)
library(dbplot)
library(bigrquery)
library(dplyr, warn.conflicts = FALSE)
library(dbplyr, warn.conflicts = FALSE)
library(modeldb)
library(leaflet)
con <- dbConnect(
bigrquery::bigquery(),
project = "publicdata",
dataset = "samples",
billing = 'tony-229812'
)
con
dbListTables(con)
library(bigrquery)
project <- "tony-229812" # put your project ID here
sql <- "SELECT year, month, day, weight_pounds FROM [publicdata:samples.natality] LIMIT 5"
query_exec(sql, project = project)
library(bigrquery)
project <- "tony-229812" # put your project ID here
sql <- "SELECT year, month, day, weight_pounds FROM [publicdata:samples.natality] LIMIT 5"
query_exec(sql, project = project)
library(odbc)
con <- dbConnect(odbc::odbc(),
.connection_string = "Driver={SQLite Driver};Database=data/chinook.db",
timeout = 10)
library("RSQLite")
db <- dbConnect(SQLite(), dbname='Test.sqlite')
dbWriteTable(db, "mtcars", mtcars)
dbWriteTable(db, "iris", iris)
dbListTables(db)
mtcars_df=dbGetQuery(db, 'SELECT * FROM mtcars LIMIT 5')
mtcars_df
library(DBI)
con <- dbConnect(odbc::odbc(), .connection_string = "Driver={SQL Server};", timeout = 10)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
# load data
library(tidyverse)
country003=read.csv('country003.csv')
country004=country003%>%group_by(Country)%>%summarise(downloads=sum(downloads))%>%
top_n(10, wt = downloads)
p=country003%>%filter(Country %in% country004$Country)%>%
plot_ly(x = ~downloads, y = ~fct_reorder(Country, downloads)) %>%
add_markers() %>%
layout(xaxis = list(title = "downloads"),
yaxis = list(title = "Country", type = "category"))
# load data
library(tidyverse)
library(plotly)
country003=read.csv('country003.csv')
country004=country003%>%group_by(Country)%>%summarise(downloads=sum(downloads))%>%
top_n(10, wt = downloads)
p=country003%>%filter(Country %in% country004$Country,year==2018)%>%
plot_ly(x = ~downloads, y = ~fct_reorder(Country, downloads)) %>%
add_markers() %>%
layout(xaxis = list(title = "downloads"),
yaxis = list(title = "Country", type = "category"))
p%>%layout(title = '2018 R base total download volume by Top 10 country')
# load data
library(tidyverse)
library(plotly)
country003=read.csv('country003.csv')
country003%>%filter(year==2018)%>%summarise(downloads=sum(downloads))
country004=country003%>%group_by(Country)%>%summarise(downloads=sum(downloads))%>%
top_n(10, wt = downloads)
p=country003%>%filter(Country %in% country004$Country,year==2018)%>%
plot_ly(x = ~downloads, y = ~fct_reorder(Country, downloads)) %>%
add_markers() %>%
layout(xaxis = list(title = "downloads"),
yaxis = list(title = "Country", type = "category"))
p%>%layout(title = '2018 R base total download volume by Top 10 country')
knitr::opts_chunk$set(echo = TRUE)
# load data
library(tidyverse)
library(plotly)
country003=read.csv('country003.csv')
# load data
library(tidyverse)
library(plotly)
country003=read.csv('country003.csv')
# load data
library(tidyverse)
library(plotly)
country003=read.csv('country003.csv')
country003%>%filter(year==2018)%>%summarise(downloads=sum(downloads))
country003%>%filter(year==2017)%>%summarise(downloads=sum(downloads))
CRAN_page <- function(...) {
file.path('https://cran.rstudio.com/src/contrib', ...)
}
parse_apache_directory_listing <- function(url) {
rbindlist(lapply(
htmllistparse$fetch_listing(url)[[2]], function(item) data.table(
Name = item$name,
`Last modified` = as.POSIXct(time$strftime('%Y-%m-%d %H:%M:%S', item$modified)),
Size = ifelse(is.null(item$size), 0, item$size))))
}
## we love data.table
library(data.table)
## get list of currently available packages on CRAN
library(reticulate)
use_python('/usr/bin/python3', required = TRUE)
htmllistparse <- import('htmllistparse')
time <- import('time')
pkgs <- parse_apache_directory_listing(CRAN_page())
## drop directories
pkgs <- pkgs[Size != 0]
## drop files that does not seem to be R packages
pkgs <- pkgs[grep('tar.gz$', Name)]
## package name should contain only (ASCII) letters, numbers and dot
pkgs[, name := sub('^([a-zA-Z0-9\\.]*).*', '\\1', Name)]
## grab date from last modified timestamp
pkgs[, date := as.character(`Last modified`)]
## keep date and name
pkgs <- pkgs[, .(name, date)]
## list of packages with at least one archived version
archives <- parse_apache_directory_listing(CRAN_page('Archive'))
## keep directories
archives <- archives[grep('/$', Name)]
## add packages not found in current list of R packages
archives[, Name := sub('/$', '', Name)]
pkgs <- rbind(pkgs,
archives[!Name %in% pkgs$name, .(name = Name)],
fill = TRUE)
## reorder pkg in alphabet order
setorder(pkgs, name)
## number of versions released is 1 for published packages
pkgs[, versions := 0]
pkgs[!is.na(date), versions := 1]
## mark archived pacakges
pkgs[, archived := FALSE]
pkgs[name %in% archives$Name, archived := TRUE]
## NA date of packages with archived versions
pkgs[archived == TRUE, date := NA]
## lookup release date of first version & number of releases
saveRDS(pkgs, '/tmp/pkgs.RDS')
pkgs[is.na(date), c('date', 'versions') := {
cat(name, '\n')
pkgarchive <- parse_apache_directory_listing(CRAN_page('Archive', name))
list(as.character(min(pkgarchive$`Last modified`)), versions + nrow(pkgarchive))
}, by = name]
## rename cols
setnames(pkgs, 'date', 'first_release')
## order by date & alphabet
setorder(pkgs, first_release, name)
pkgs[, index := .I]
pkgs[c(250, 500, (1:13)*1000)]
##              name       first_release versions archived index
##  1:         gstat 2003-02-04 15:24:00      111     TRUE   250
##  2:       relsurv 2005-01-21 12:52:00       33     TRUE   500
##  3:      spsurvey 2007-01-24 11:07:00       16     TRUE  1000
##  4:          oosp 2009-08-20 13:48:00        6     TRUE  2000
##  5:  penalizedLDA 2011-03-29 18:20:00        2     TRUE  3000
##  6:          R330 2012-06-07 06:08:00        1    FALSE  4000
##  7:     lbiassurv 2013-03-11 06:39:00        2     TRUE  5000
##  8:      gconcord 2014-01-24 17:57:00        2     TRUE  6000
##  9:        segmag 2014-10-17 08:30:00        3     TRUE  7000
## 10:          BCEE 2015-06-24 16:22:00        3     TRUE  8000
## 11: ontologyIndex 2016-01-11 14:32:00        5     TRUE  9000
## 12:     europepmc 2016-07-13 08:19:00        7     TRUE 10000
## 13:      magicfor 2016-12-18 10:23:00        1    FALSE 11000
## 14:         wally 2017-05-17 07:50:00        2     TRUE 12000
## 15:          xSub 2017-10-11 17:27:00        2     TRUE 13000
## plot trend
library(ggplot2)
ggplot(pkgs, aes(as.Date(first_release), index)) +
geom_line(size = 2) +
scale_x_date(date_breaks = '1 year', date_labels = '%Y') +
scale_y_continuous(breaks = seq(0, 16000, 1000)) +
xlab('') + ylab('') + theme_bw() +
ggtitle('Number of R packages ever published on CRAN')
time <- import('time')
pkgs <- parse_apache_directory_listing(CRAN_page())
parse_apache_directory_listing <- function(url) {
rbindlist(lapply(
htmllistparse$fetch_listing(url)[[2]], function(item) data.table(
Name = item$name,
`Last modified` = as.POSIXct(time$strftime('%Y-%m-%d %H:%M:%S', item$modified)),
Size = ifelse(is.null(item$size), 0, item$size))))
}
parse_apache_directory_listing <- function(url) {
rbindlist(lapply(
htmllistparse$fetch_listing(url)[[2]], function(item) data.table(
Name = item$name,
`Last modified` = as.POSIXct(time$strftime('%Y-%m-%d %H:%M:%S', item$modified)),
Size = ifelse(is.null(item$size), 0, item$size))))
}
## we love data.table
library(data.table)
library(data.table)
## get list of currently available packages on CRAN
library(reticulate)
htmllistparse <- import('htmllistparse')
time <- import('time')
pkgs <- parse_apache_directory_listing(CRAN_page())
htmllistparse <- import('htmllistparse')
htmllistparse <- import('htmllistparse')
## get list of currently available packages on CRAN
library(reticulate)
htmllistparse <- import('htmllistparse')
extract_url <- function(){
url <- list(
archive = "https://cran-archive.r-project.org/bin/windows/contrib/",
active  = "https://cran.r-project.org/bin/windows/contrib/"
)
extract_url <- function(){
url <- list(
archive = "https://cran-archive.r-project.org/bin/windows/contrib/",
active  = "https://cran.r-project.org/bin/windows/contrib/"
)
get_urls <- function(url){
txt <- readLines(url)
idx <- grep("\\d.\\d+/", txt)
txt[idx]
versions <- gsub(".*?>(\\d.\\d+(/)).*", "\\1", txt[idx])
versions
paste0(url, versions)
}
z <- lapply(url, get_urls)
unname(unlist(z))
}
# Given a CRAN URL, extract the number of packages and date
extract_pkg_info <- function(url){
extract_date <- function(txt, fun = max){
txt <- txt[-grep("[(STATUS)|(PACKAGES)](.gz)*", txt)]
pkgs <- grep(".zip", txt)
txt <- txt[pkgs]
ptn <- ".*?>(\\d{2}-...-\\d{4}).*"
idx <- grep(ptn, txt)
date <- gsub(ptn, "\\1", txt[idx])
date <- as.Date(date, format = "%d-%b-%Y")
match.fun(fun)(date)
}
message(url)
txt <- readLines(url)
count <- length(grep(".zip", txt))
# sum(grepl(".zip", txt))
# head(txt)
data.frame(
version = basename(url),
date = extract_date(txt),
pkgs = count
)
}
# Get the list of CRAN URLs
CRAN_urls <- extract_url()
CRAN_urls
# Extract package information
pkgs <- lapply(CRAN_urls, extract_pkg_info)
pkgs <- do.call(rbind, pkgs)
head(pkgs)
tail(pkgs)
pkgs <- head(pkgs, -2) # Remove r-devel and r-future
# Extract major release information
major_releases <- pkgs[grep("\\.0", pkgs$version), ]
#
library(ggplot2)
p <- ggplot(pkgs, aes(x = date, y = pkgs)) +
geom_smooth() +
geom_point() +
geom_rug(colour = "grey50") +
geom_vline(data = major_releases,
aes(xintercept = as.numeric(date)),
colour = "grey80") +
geom_text(data = major_releases,
aes(label = paste("Version", version), y = 8000),
angle = 90,
colour = "red",
hjust = 1, vjust = -1) +
theme_minimal(16) +
ggtitle("Number of CRAN packages per R version") +
xlab(NULL) +
ylab(NULL)
print(p)
extract_url <- function(){
url <- list(
archive = "https://cran-archive.r-project.org/bin/windows/contrib/",
active  = "https://cran.r-project.org/bin/windows/contrib/"
)
get_urls <- function(url){
txt <- readLines(url)
idx <- grep("\\d.\\d+/", txt)
txt[idx]
versions <- gsub(".*?>(\\d.\\d+(/)).*", "\\1", txt[idx])
versions
paste0(url, versions)
}
z <- lapply(url, get_urls)
unname(unlist(z))
}
# Given a CRAN URL, extract the number of packages and date
extract_pkg_info <- function(url){
extract_date <- function(txt, fun = max){
txt <- txt[-grep("[(STATUS)|(PACKAGES)](.gz)*", txt)]
pkgs <- grep(".zip", txt)
txt <- txt[pkgs]
ptn <- ".*?>(\\d{2}-...-\\d{4}).*"
idx <- grep(ptn, txt)
date <- gsub(ptn, "\\1", txt[idx])
date <- as.Date(date, format = "%d-%b-%Y")
match.fun(fun)(date)
}
message(url)
txt <- readLines(url)
count <- length(grep(".zip", txt))
# sum(grepl(".zip", txt))
# head(txt)
data.frame(
version = basename(url),
date = extract_date(txt),
pkgs = count
)
}
# Get the list of CRAN URLs
CRAN_urls <- extract_url()
CRAN_urls
# Extract package information
pkgs <- lapply(CRAN_urls, extract_pkg_info)
pkgs <- do.call(rbind, pkgs)
head(pkgs)
tail(pkgs)
pkgs <- head(pkgs, -2) # Remove r-devel and r-future
# Extract major release information
major_releases <- pkgs[grep("\\.0", pkgs$version), ]
#
library(ggplot2)
p <- ggplot(pkgs, aes(x = date, y = pkgs)) +
geom_smooth() +
geom_point() +
geom_rug(colour = "grey50") +
geom_vline(data = major_releases,
aes(xintercept = as.numeric(date)),
colour = "grey80") +
geom_text(data = major_releases,
aes(label = paste("Version", version), y = 8000),
angle = 90,
colour = "red",
hjust = 1, vjust = -1) +
theme_minimal(16) +
ggtitle("Number of CRAN packages per R version") +
xlab(NULL) +
ylab(NULL)
print(p)
)
extract_url <- function(){
url <- list(
archive = "https://cran-archive.r-project.org/bin/windows/contrib/",
active  = "https://cran.r-project.org/bin/windows/contrib/"
)
get_urls <- function(url){
txt <- readLines(url)
idx <- grep("\\d.\\d+/", txt)
txt[idx]
versions <- gsub(".*?>(\\d.\\d+(/)).*", "\\1", txt[idx])
versions
paste0(url, versions)
}
z <- lapply(url, get_urls)
unname(unlist(z))
}
# Given a CRAN URL, extract the number of packages and date
extract_pkg_info <- function(url){
extract_date <- function(txt, fun = max){
txt <- txt[-grep("[(STATUS)|(PACKAGES)](.gz)*", txt)]
pkgs <- grep(".zip", txt)
txt <- txt[pkgs]
ptn <- ".*?>(\\d{2}-...-\\d{4}).*"
idx <- grep(ptn, txt)
date <- gsub(ptn, "\\1", txt[idx])
date <- as.Date(date, format = "%d-%b-%Y")
match.fun(fun)(date)
}
message(url)
txt <- readLines(url)
count <- length(grep(".zip", txt))
# sum(grepl(".zip", txt))
# head(txt)
data.frame(
version = basename(url),
date = extract_date(txt),
pkgs = count
)
}
# Get the list of CRAN URLs
CRAN_urls <- extract_url()
CRAN_urls
# Extract package information
pkgs <- lapply(CRAN_urls, extract_pkg_info)
pkgs <- do.call(rbind, pkgs)
head(pkgs)
tail(pkgs)
pkgs <- head(pkgs, -2) # Remove r-devel and r-future
# Extract major release information
major_releases <- pkgs[grep("\\.0", pkgs$version), ]
#
library(ggplot2)
p <- ggplot(pkgs, aes(x = date, y = pkgs)) +
geom_smooth() +
geom_point() +
geom_rug(colour = "grey50") +
geom_vline(data = major_releases,
aes(xintercept = as.numeric(date)),
colour = "grey80") +
geom_text(data = major_releases,
aes(label = paste("Version", version), y = 8000),
angle = 90,
colour = "red",
hjust = 1, vjust = -1) +
theme_minimal(16) +
ggtitle("Number of CRAN packages per R version") +
xlab(NULL) +
ylab(NULL)
print(p)
p
glimpse(pkgs)
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
library(leaflet)
library(ggmap)
location=geocode("WUYI", source = "dsk")
lon=location$lon
lat=location$lat
m <- leaflet()%>%
addTiles() %>%  # Add default OpenStreetMap map tiles
setView(lng=lon, lat=lat, zoom = 05) %>%
addMarkers(lng=lon, lat=lat, popup="WUYI mountains")
m
library(leaflet)
library(ggmap)
location=geocode("WUYI", source = "dsk")
lon=location$lon
lat=location$lat
m <- leaflet()%>%
addTiles() %>%  # Add default OpenStreetMap map tiles
setView(lng=lon, lat=lat, zoom = 08) %>%
addMarkers(lng=lon, lat=lat, popup="WUYI mountains")
m
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
