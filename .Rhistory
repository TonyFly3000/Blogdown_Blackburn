set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(ramen_ratings_02), size = floor(.9*nrow(ramen_ratings_02)), replace = F)
train <- ramen_ratings_02[sample, ]
test  <- ramen_ratings_02[-sample, ]
dim(ramen_ratings_02)
dim(train)
dim(test)
tree = rpart(stars_target ~ country+style+brand_top, data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
test$predict_stars_target<-predict(tree, test, type = 'class')
table_mat <- table(test$stars_target, test$predict_stars_target)
table_mat
test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
test%>%count(stars_target)%>%mutate(percent = n / sum(n))
train$predict_stars_target<-predict(tree, train, type = 'class')
table_mat <- table(train$stars_target, train$predict_stars_target)
table_mat
train%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
blogdown:::serve_site()
ramen_ratings_02%>%count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
ramen_ratings_02%>%count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50))
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
ramen_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv")
glimpse(ramen_ratings)
names(ramen_ratings%>%select_if(is.numeric))
names(ramen_ratings%>%select_if(is.character))
ramen_ratings %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
ramen_ratings %>%mutate(brand=fct_lump(brand,10))%>%
count(brand, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%
count(style, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%mutate(country=fct_lump(country,8))%>%
count(country, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%mutate(country=fct_lump(country,20))%>%
count(brand,country, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings%>%filter(country=='Hong Kong')
ramen_ratings%>%arrange(desc(stars))
ramen_ratings_02%>%count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50))
ramen_ratings_02%>%count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
View(ramen_ratings)
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy_flag=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') ,
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken'),
Laksa_flag=str_detect(variety,'Laksa')
)
ramen_ratings_02 %>%
count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings_02 %>%
count(stars_level, sort = TRUE)%>%mutate(percent = n / sum(n))
set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(ramen_ratings_02), size = floor(.9*nrow(ramen_ratings_02)), replace = F)
train <- ramen_ratings_02[sample, ]
test  <- ramen_ratings_02[-sample, ]
dim(ramen_ratings_02)
dim(train)
dim(test)
dim(ramen_ratings_02)
dim(train)
dim(test)
glimpse(train)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(ramen_ratings_02), size = floor(.9*nrow(ramen_ratings_02)), replace = F)
train <- ramen_ratings_02[sample, ]%>%select(-variety,-stars,-stars_level,-brand)
test  <- ramen_ratings_02[-sample, ]%>%select(-variety,-stars,-stars_level,-brand)
dim(ramen_ratings_02)
dim(train)
dim(test)
glimpse(train)
tree = rpart(stars_target ~ country+style+brand_top, data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
train$predict_stars_target<-predict(tree, train, type = 'class')
table_mat <- table(train$stars_target, train$predict_stars_target)
table_mat
train%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
tree = rpart(stars_target ~ ., data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
tree = rpart(stars_target ~ country+style+brand_top+Spicy_flag+Beef_flag+Udon_flag+Chicken_flag+Laksa_flag, data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
train$predict_stars_target<-predict(tree, train, type = 'class')
table_mat <- table(train$stars_target, train$predict_stars_target)
table_mat
train%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
ramen_ratings <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv")
glimpse(ramen_ratings)
names(ramen_ratings%>%select_if(is.numeric))
names(ramen_ratings%>%select_if(is.character))
ramen_ratings %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~ key, scales = "free") +
geom_histogram()
ramen_ratings %>%mutate(brand=fct_lump(brand,10))%>%
count(brand, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%
count(style, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%mutate(country=fct_lump(country,8))%>%
count(country, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings %>%mutate(country=fct_lump(country,20))%>%
count(brand,country, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings%>%filter(country=='Hong Kong')
ramen_ratings%>%arrange(desc(stars))
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy_flag=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') ,
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken'),
Laksa_flag=str_detect(variety,'Laksa')
)
ramen_ratings_02 %>%
count(stars_target, sort = TRUE)%>%mutate(percent = n / sum(n))
ramen_ratings_02 %>%
count(stars_level, sort = TRUE)%>%mutate(percent = n / sum(n))
data=ramen_ratings_02%>%mutate(country=fct_lump(country,8))
ggplot(data, aes(x = country, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(ramen_ratings_02), size = floor(.9*nrow(ramen_ratings_02)), replace = F)
train <- ramen_ratings_02[sample, ]%>%select(-variety,-stars,-stars_level,-brand)
test  <- ramen_ratings_02[-sample, ]%>%select(-variety,-stars,-stars_level,-brand)
dim(ramen_ratings_02)
dim(train)
dim(test)
glimpse(train)
tree = rpart(stars_target ~ country+style+brand_top+Spicy_flag, data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
train$predict_stars_target<-predict(tree, train, type = 'class')
table_mat <- table(train$stars_target, train$predict_stars_target)
table_mat
train%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy_flag=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken')
)
set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(ramen_ratings_02), size = floor(.9*nrow(ramen_ratings_02)), replace = F)
train <- ramen_ratings_02[sample, ]%>%select(-variety,-stars,-stars_level,-brand)
test  <- ramen_ratings_02[-sample, ]%>%select(-variety,-stars,-stars_level,-brand)
dim(ramen_ratings_02)
dim(train)
dim(test)
glimpse(train)
tree = rpart(stars_target ~ country+style+brand_top+Spicy_flag, data=train,method = 'class')
rpart.plot(tree)
prp(tree)
#tree
train$predict_stars_target<-predict(tree, train, type = 'class')
table_mat <- table(train$stars_target, train$predict_stars_target)
table_mat
train%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
data=ramen_ratings_02%>%
ggplot(data, aes(x = Spicy_flag , fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy_flag=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken')
)
data=ramen_ratings_02%>%
ggplot(data, aes(x = Spicy_flag , fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02%>%mutate(country=fct_lump(country,8))
ggplot(data, aes(x = country, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar()+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Spicy_flag=if_else(Spicy>0,1,0)
Beef_flag=str_detect(variety,'Beef'),
ramen_ratings_02=ramen_ratings%>%filter(is.na(stars)==FALSE)%>%
mutate(stars_level=factor(round(stars)),stars_target=factor(if_else(stars>=4,1,0)))%>%select(-review_number)%>%
mutate(brand_top=fct_lump(brand,50),
Spicy=str_detect(variety,'Spicy')+str_detect(variety,'Chili')+str_detect(          variety,'Hot')+str_detect(variety,'Kimchi') +str_detect(variety,'Laksa'),
Spicy_flag=if_else(Spicy>0,1,0),
Beef_flag=str_detect(variety,'Beef'),
Udon_flag=str_detect(variety,'Udon'),
Chicken_flag=str_detect(variety,'Chicken')
)
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar()+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
data=ramen_ratings_02
ggplot(data, aes(x = Spicy_flag, fill = stars_level))+
geom_bar(position="fill", colour="black")+
theme_classic(base_size = 9)+
labs(title='Bar chart for country due to stars ranges',
subtitle='Cumulated', y="Indicator", fill="User assessment ranges")
test$predict_stars_target_prob<-predict(tree, test)
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[1]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,1]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,2]
test$predict_stars_target_prob
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test
test$predict_stars_target<-predict(tree, test, type = 'class')
table_mat <- table(test$stars_target, test$predict_stars_target)
table_mat
test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
test
test$predict_stars_target_prob<-predict(tree, test)[,2]
test=test%>%arrange(desc(predict_stars_target_prob))
top_50_test=head(test,50)
top_50_test%>%mutate(true=if_else(stars_target==predict_stars_target,1,0))%>%group_by()%>%summarise(true=sum(true),total=n())%>%mutate(accuary_rate=true/total)
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
library("ROCR")
# input data
library(readr)
# clean data
library(tidyverse)
# model
library(rpart)
library(rpart.plot)
library(ROCR)
pred <- prediction(predict(tree, type = "prob")[, 2], test$stars_target)
Pred.cart = predict(tree, newdata = test, type = "prob")[,2]
Pred2 = prediction(Pred.cart, test$stars_target)
plot(performance(Pred2, "tpr", "fpr"))
abline(0, 1, lty = 2)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
getwd()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/public/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("./public/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv(".post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("./post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("post/data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post\data-weekend-global-plastic-waste/global-plastics-production.csv")
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("/post/data-weekend-global-plastic-waste/global-plastics-production.csv")
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
blogdown:::serve_site()
blogdown:::serve_site()
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
library(tidyverse)
library(dplyr)
library(scales)
library(ggplot2)
library(readr)
library(janitor)
library(ggthemes)
theme_set(theme_light())
# data source shttps://ourworldindata.org/plastic-pollution
global_plastics_production=read_csv("https://github.com/TonyFly3000/kaggle/blob/master/global-plastics-production.csv")
plastic_waste <- clean_dataset(coast_vs_waste) %>%  # 清洗数据 coast_vs_waste
select(-total_population_gapminder) %>%           # 删除total_population_gapminder变量
inner_join(clean_dataset(mismanaged_vs_gdp) %>%   # 清洗数据 mismanaged_vs_gdp
select(-total_population_gapminder), # 删除total_population_gapminder变量
by = c("country", "country_code")) %>% # inner join by "country", "country_code"
inner_join(clean_dataset(waste_vs_gdp),           # 清洗数据 waste_vs_gdp
by = c("country", "country_code")) %>% # inner join by "country", "country_code"
# 选变量
select(country,
country_code,
mismanaged_waste = mismanaged_plastic_waste_tonnes,
coastal_population,
total_population = total_population_gapminder,
mismanaged_per_capita = per_capita_mismanaged_plastic_waste_kilograms_per_person_per_day,
plastic_waste_per_capita=per_capita_plastic_waste_kilograms_per_person_per_day,
gdp_per_capita = gdp_per_capita_ppp_constant_2011_international_rate) %>%
mutate(gdp_per_capita=round(gdp_per_capita))%>%
filter(!is.na(mismanaged_waste),country!='Trinidad and Tobago')%>%                 # 选mismanaged_waste 非空的记录
mutate(gdp_per_capita_group=cut_number(gdp_per_capita/1000, n = 4) # 按gdp_per_capita 排序 将国家分为 4组
,mismanaged_per_capita_rate=mismanaged_per_capita/plastic_waste_per_capita,
managed_per_capita_rate=1-mismanaged_per_capita_rate
)
global_plastics_production%>%clean_names() %>%filter(year>2000) %>%
ggplot(aes(x=year,y=global_plastics_production_million_tonnes_tonnes/1000000)) +geom_point()+geom_line()+
labs(x = "年",
y = "全球制造的塑料垃圾（百万吨）",
#color = "Coastal population",
title = "人类制造越来越多的垃圾",
subtitle = "每年制造近4亿万吨塑料垃圾",
caption ="统计时间:2000-2015年;数据源:ourworldindata.org
@Tony Duan"
)+theme(plot.title = element_text(hjust = 0.5))
global_plastics_production
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
library(keras)
library(reticulate)
tensorflow::tf_config()
keras:::keras_version()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
par(mfcol=c(4,4))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:16) {
img <- x_train[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = y_train[i])
}
img <- x_train[1, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n')
x_train[1, , ][1]
x_train[1, , ][1:784]
length(x_train[1, , ])
x_train[1, , ]
200/255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
dim(x_train)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
dim(y_train)
head(y_train)
?compile
summary(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
256*784
256*128
255*784
257*784
256*785
128*785
257*128
200960+32896+1290
784*256
200960-200704
128*10
blogdown:::serve_site()
